Emotion Classification Using Facial Expressions and EEG

This repository contains all code, data pipelines, models, and notebooks developed as part of the IITB EdTech Internship 2025 (Group T1_G38 ‚Äì Team_Attackers) for Problem ID 4: Emotion Classification using Facial Expressions and EEG.
üìë Project Overview

Objective: Predict emotions (e.g. Engaged, Confused, Neutral) from EEG and Affectiva facial expression data.

Modalities:

EEG (Delta, Theta, Alpha, Beta, Gamma bands)

Facial Expressions (Affectiva probabilities & Action Units from TIVA)

Key Challenges: Temporal alignment of multimodal data and class imbalance.

project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ EEG_combined.csv # Contains preprocessed and synchronized EEG features.
‚îÇ   ‚îú‚îÄ‚îÄ PSY_combined.csv # Contains preprocessed and synchronized TIVA behavioral data.
‚îÇ   ‚îú‚îÄ‚îÄ TIVA_combined.csv # Contains preprocessed and synchronized Affectiva facial features.
‚îÇ   ‚îú‚îÄ‚îÄ WINDOW_combined.csv # The core dataset after applying the sliding window approach.
‚îÇ   ‚îî‚îÄ‚îÄ df_features_trials.csv # A trial-level dataset with final labels and features.
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_preprocessing.ipynb # Notebook for data cleaning, merging, and initial synchronization.
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb # Notebook for extracting and combining multimodal features.
‚îÇ   ‚îú‚îÄ‚îÄ 03_modeling_baseline.ipynb # Notebook for training and evaluating baseline ML models (RF, XGBoost, LR).
‚îÇ   ‚îú‚îÄ‚îÄ 04_temporal_models.ipynb # Notebook dedicated to implementing and training the BiLSTM with attention.
‚îÇ   ‚îú‚îÄ‚îÄ 05_modeling_fusion.ipynb # Notebook for exploring different fusion techniques (early, late, intermediate).
‚îÇ   ‚îî‚îÄ‚îÄ 06_analysis.ipynb # Notebook for in-depth model evaluation, interpretation, and visualization.
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ rf_allfeatures.pkl # A trained Scikit-learn Random Forest model.
‚îÇ   ‚îú‚îÄ‚îÄ xg_allfeatures.pkl # A trained Scikit-learn XGBoost model.
‚îÇ   ‚îú‚îÄ‚îÄ logreg_scalefeatures.pkl # A trained Scikit-learn Logistic Regression model.
‚îÇ   ‚îú‚îÄ‚îÄ rf_eeg.pkl # A trained Random Forest model using only EEG features.
‚îÇ   ‚îú‚îÄ‚îÄ rf_tiva_latefusion.pkl # A trained Random Forest model using only TIVA features for late fusion.
‚îÇ   ‚îú‚îÄ‚îÄ rf_eeg_latefusion.pkl # A trained Random Forest model using only EEG features for late fusion.
‚îÇ   ‚îú‚îÄ‚îÄ rf_earlyfusion.pkl # A trained Random Forest model on concatenated EEG and TIVA features.
‚îÇ   ‚îú‚îÄ‚îÄ mlp_intermidatefusion.pkl # A trained MLP model for intermediate fusion.
‚îÇ   ‚îî‚îÄ‚îÄ multimodal_bilstm_attention.pth # A trained PyTorch BiLSTM model with attention.
‚îî‚îÄ‚îÄ README.md # The project's main documentation and overview.

üóÇ Data Access
Raw and processed data are hosted on Google Drive: üì• IITB Dataset Folder

Within Drive:

IITB/Data/processed contains combined/cleaned data used in this repository.

IITB/project/ contains notebooks and scripts.

üöÄ Steps Implemented (matching internship tasks)
1Ô∏è‚É£ Preprocessing
Combined EEG, TIVA, PSY data across 38 students.

Cleaned missing values & handled NaNs.

Synchronized timestamps using routineStart/routineEnd.

Downsampled to ~2.3 Hz per label.

2Ô∏è‚É£ Feature Engineering
EEG: mean/variance for Delta, Theta, Alpha, Beta, Gamma; frontal asymmetry, engagement index.

Facial: raw Affectiva emotion probabilities (Joy, Anger, Fear‚Ä¶) and Action Units (AUs) aggregated per trial (mean/std/max/occurrence).

3Ô∏è‚É£ Modeling
Baseline (03_modeling_baseline): RF, XGBoost, Logistic Regression on concatenated EEG+TIVA features. The saved models include rf_allfeatures.pkl, xg_allfeatures.pkl, and logreg_scalefeatures.pkl.

Fusion (05_modeling_fusion):

Early fusion: concatenate EEG+facial features ‚Üí classifier. The model rf_earlyfusion.pkl is an example of this approach.

Late fusion: separate models per modality, combine logits. The models rf_eeg_latefusion.pkl and rf_tiva_latefusion.pkl represent this approach.

Intermediate fusion: embeddings from CNN + EEG MLP ‚Üí joint classifier. The saved model is mlp_intermidatefusion.pkl.

Advanced: BiLSTM with attention for temporal modeling, saved as multimodal_bilstm_attention.pth. This is implemented in the 04_temporal_models.ipynb notebook.

4Ô∏è‚É£ Evaluation & Interpretation (06_analysis)
Accuracy, macro F1, precision, recall.

Confusion Matrix per class.

ROC-AUC for binary subsets (e.g., Engaged vs Not Engaged).

SHAP values to interpret EEG bandpower and AUs importance.

Attention weights visualization to see modality dominance.

5Ô∏è‚É£ Experimentation & Improvement
Tested different time-window sizes.

Oversampling with SMOTE for underrepresented emotions.

Participant-specific vs cross-participant splits.

Continuous dimensions (Valence, Arousal).

Temporal smoothing of predictions (HMM / majority vote).

üìä How to Reproduce
Clone this repo.

Install requirements:

Bash

pip install -r requirements.txt
This will install pandas, numpy, scikit-learn, torch, xgboost, and other necessary libraries.

Mount your Google Drive or ensure data/ and models/ are in place.

Run notebooks in order:

01_preprocessing.ipynb -> 02_feature_engineering.ipynb -> 03_modeling_baseline.ipynb -> 04_temporal_models.ipynb -> 05_modeling_fusion.ipynb -> 06_analysis.ipynb

Models are saved automatically in models/ after training:
import joblib
rf = joblib.load('models/rf_allfeatures.pkl')

For PyTorch models:
import torch
# Replace MyBiLSTMModel with your actual model class
model = MyBiLSTMModel(...)
model.load_state_dict(torch.load('models/multimodal_bilstm_attention.pth'))
model.eval()

## üìà Results (Test Set)

| Model                          | Accuracy | Macro F1 |
|-------------------------------|----------|----------|
| Random Forest (All Features)  | 0.61     | 0.28     |
| XGBoost (All Features)        | 0.64     | 0.27     |
| Logistic Regression (Scaled)  | 0.41     | 0.25     |
| Random Forest (EEG only)      | 0.71     | 0.21     |
| Random Forest (Early Fusion)  | 0.60     | 0.28     |
| Random Forest (Late Fusion)   | 0.72     | 0.21     |
| MLP (Intermediate Fusion)     | 0.61     | 0.26     |
| BiLSTM + Attention            | 0.60     | 0.25     |

üìù Notes

All .pkl models use joblib .

PyTorch models are saved as .pth.


üë• Team

Group T1_G38 ‚Äì Team_Attackers

Group Members:1)Pranav Chandrakant Patil
              2)Harshad Balaso Kanire
              3)Omkar Manik patil
Faculty Mentor: Mr. Swapnil T. Powar
